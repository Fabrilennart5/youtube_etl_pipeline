# YouTube ETL Pipeline üöÄ

![Imagen para el proyecto](https://pbs.twimg.com/media/GoXR7xuWsAA5q0U?format=jpg&name=900x900)

**Pipeline ETL automatizado para extracci√≥n de datos de YouTube** que utiliza Apache Airflow y Docker. Dise√±ado para obtener estad√≠sticas de canales/videos, procesarlas y almacenarlas en SQLite.

---

## üìå Requisitos

Para ejecutar este proyecto, aseg√∫rate de tener lo siguiente:

1. **API Key de YouTube**: Obt√©n una API Key v√°lida desde Google Cloud Platform para acceder a la API de YouTube.
2. **ID del canal de YouTube**: Identifica el canal del cual deseas extraer datos. (Ej: `UC_x5XG1OV2P6uZZ5FSM9Ttw`)
3. **Docker**: Necesitas Docker y Docker Compose instalados en tu sistema.
4. **uv**: Para la gesti√≥n de dependencias en un entorno local.

---

## üõ†Ô∏è Instalaci√≥n

Sigue estos pasos para instalar y configurar el proyecto:

Sigue estos pasos para instalar y configurar el proyecto:

1. **Clona el repositorio**:
   ```bash
   git clone git@github.com:Fabrilennart5/youtube_etl_pipeline.git
   cd youtube_etl_pipeline


2. **Configura la API Key y el ID del canal**:
    
    ```bash
    echo "API_KEY=tu_api_key_aqui" > .env
    echo "CHANNEL_ID=id_del_canal" >> .env
    ```
    
3. **Configura Docker (opcional)**:
    
    ```bash
    docker-compose build
    ```
    
4. **Levanta el entorno Docker**:
    
    ```bash
    docker-compose up -d
    ```
    
5. **Instalaci√≥n local con uv**:
    
    ```bash
    uv venv
    source .venv/bin/activate
    uv pip install -r requirements.txt
    ```

## üöÄ Ejecuci√≥n

### Con Docker:

1. Accede a la interfaz de Airflow en `http://localhost:8080`
    
2. Activa el DAG `youtube_etl_dag`
    
3. Verifica los datos en `data/processed/`
    

### Localmente:

```bash
python dags/youtube_etl_dag.py
```

### Estructura de salida:

```bash
data/
‚îú‚îÄ‚îÄ raw/              # JSON crudos
‚îú‚îÄ‚îÄ processed/        # CSV transformados
‚îî‚îÄ‚îÄ data_base/        # SQLite con datos finales
```

---

## ‚öôÔ∏è Configuraci√≥n Adicional

- **Variables de entorno**: Configura `API_KEY` y `CHANNEL_ID` en `.env`
    
- **Puertos**: Verifica disponibilidad de puertos (8080 para Airflow)
    
- **Logs**: Revisa los logs en `logs/` para diagn√≥stico de errores
    

---

## üîÑ Flujo del Pipeline

1. **Extracci√≥n**: Obtiene estad√≠sticas del canal mediante YouTube API
    
2. **Transformaci√≥n**: Convierte JSON a CSV estructurado
    
3. **Carga**: Almacena en SQLite con metadatos de ejecuci√≥n
    

---

## ‚ñ∂Ô∏è Tutorial de Funcionamiento

**Video explicativo del proyecto**:  
[![DuckDB + ETL con Python](https://img.shields.io/badge/Ver_Tutorial-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.linkedin.com/posts/fabricio-lennart_duckdb-etl-python-activity-7316911074480447488-9C76?utm_source=share&utm_medium=member_desktop&rcm=ACoAACJ0kvIB-RfjThNzVfvOPXGuLylc2ySNWLY)



